README: BUILDING GENASIS APPLICATIONS, EXAMPLES, AND/OR UNIT TESTS
--------------------------------------------------------------------------

A machine-specific Makefile is needed to build GenASiS programs. Several 
sample Makefiles are provided under the subdirectory Build/Machines. 
Minor modifications of one of the provided Makefiles that most
approximates one's computing environment is often sufficient to get started. 
The essential information needed includes the name of the compiler wrapper
to compile MPI-based code (e.g. commonly 'mpif90' for Fortran), 
compiler-specific flags for various debugging and optimization options, and
the flags and locations to include and link with required third-party I/O 
libraries Silo. 

This version is compatible with Silo-4.10 and above. To use Silo with 
GenASiS, the Fortran interface to Silo should be enabled (which it is by
default). In the simplest case, one can build Silo to be used with GenASiS
with the following commands inside the Silo distribution:
> ./configure --enable-fortran
> make
> make install
Silo's documentation (e.g. its INSTALL file) provides more complete
information on building Silo.

Once the machine-specific Makefile is set up, the environment variable 
GENASIS_MACHINE has to be set to tell the GenASiS build system to use the 
corresponding Makefile. For example, to use the Makefile for the GCC compiler
on a Linux machine (i.e. Makefile_Linux_GCC), in a Bash Unix shell one can
type:

> export GENASIS_MACHINE=Linux_GCC

In most common computing environments with a generic MPI library, the fluid
dynamics programs described in the accompanying paper can then be built and 
executed. For instance, these commands build all the examples:

> cd Programs/Examples/Mathematics/FluidDynamics/Executables
> make PURPOSE=OPTIMIZE all


The first few of these are run (here with 8 MPI processes) with the commands 
> export OMP_NUM_THREADS=1
> mpirun -np 8 ./SineWave_Linux_GCC nCells=128,128,128
> mpirun -np 8 ./SawtoothWave_Linux_GCC nCells=128,128,128 nWavelengths=2,2,2
> mpirun -np 8 ./RiemannProblem_Linux_GCC nCells=128,128,128 FinishTime=0.25

In most common computing environments with a generic MPI library, the
fluid dynamics example programs (described in the accompanying paper) can then
be built and executed (here with 8 MPI processes) with the following commands:

> cd Programs/Examples/Mathematics/FluidDynamics/Executables
> make PURPOSE=OPTIMIZE all
> mpirun -np 8 ./SineWave_Linux_GCC nCells=128,128,128
> mpirun -np 8 ./SawtoothWave_Linux_GCC nCells=128,128,128 \
nWavelengths=2,2,2
> mpirun -np 8 ./RiemannProblem_Linux_GCC nCells=128,128,128 \
FinishTime=0.25

(To compile in a manner that is unoptimized but useful for debuggers, 
replace 'PURPOSE=OPTIMIZE' with 'PURPOSE=DEBUG'. Or omit it altogether; 
in the absence of a specification of PURPOSE, the Makefile in 
FluidDynamics/Executables sets PURPOSE=DEBUG as a default.)
Note that in these examples, the optional non-default parameter values for
nCells, nWavelengths, and FinishTime---which were used in
generating the figures in the accompanying paper---are passed
to the programs in this case via command-line options. The 1D and 2D cases
of these programs can also be executed by specifying fewer elements for 
nCells, for example

> mpirun -np 2 ./RiemannProblem_Linux_GCC nCells=128 Dimensionality=1D \
FinishTime=0.25
> mpirun -np 4 ./RiemannProblem_Linux_GCC nCells=128,128 Dimensionality=2D \
FinishTime=0.25

The "Dimensionality" option is also used as an appendix to the name of
the output file and it shall be consistent with the number of elements given
to "nCells" to determine the desired dimensionality of the mesh.

In the above examples we explicitly set the number of OpenMP threads with
the environmental variable OMP_NUM_THREADS. It is imperative to
do so since the default number of threads varies among different compilers
if this environmental variable is not set. When running with more than one
OpenMP thread per MPI task, one must take care so that thread placement on
the processors is set correctly to avoid unintended resource contention.
 
By default the output files are written in the directory "Output"
that resides on the same level as the "Executables" directory, but
this can be changed with an optional 'OutputDirectory' command line
option. 

If the VisIt visualization package is available, plots similar to the Figures 
in the accompanying paper can be generated using the supplied visualization
script called from the "Output" directory. The script takes one argument, 
which is the program name appended with the "Dimensionality" string. Assuming 
the executable "visit" is available, the visualization script can be called, 
for example, as the follows:

> cd Programs/Examples/Mathematics/FluidDynamics/Output
> visit -cli -s ../FluidDynamics.visit.py SineWave_3D
> visit -cli -s ../FluidDynamics.visit.py RiemannProblem_2D
> visit -cli -s ../FluidDynamics.visit.py RayleighTaylor_2D

(The option "-nowin" can be added to above VisIt invocation to prevent VisIt
from trying to display its visualization windows, resulting in the image files
being drawn off-screen instead.)

Unit test programs exercising individual GenASiS classes can similarly be
built and executed inside the "Executables" directory of each leaf
division of the code under "Programs/UnitTests". For example, the following 
commands build and execute the unit test programs for classes in the 
"Fields" division:

> cd Programs/UnitTests/Mathematics/Solvers/Fields/Executables
> make all
> mpirun -np 1 [program_name]

This blanket "make all" builds all the unit test targets in the Makefile
fragment Programs/UnitTests/Mathematics/Solvers/Fields/Makefile_Fields. 
Individual targets of course also can be built.

GenASiS Mathematics has been tested with the following compilers: GCC
Fortran compiler (gfortran, part of GCC) version 6.2.0, Intel Fortran 16,
and Cray Compiler Environment version 8.5.3. Newer versions
of these compilers are likely to work as well. GenASiS Mathematics is written 
in full compliance with the Fortran 2003 standard to enhance portability.

Authors:
Christian Cardall (cardallcy@ornl.gov)
Reuben Budiardja (reubendb@ornl.gov)
